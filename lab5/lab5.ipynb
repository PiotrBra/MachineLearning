{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4471f9668b37ab8f",
   "metadata": {},
   "source": [
    "# Deep Double Descent\n",
    "Autor: Piotr Branewski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2254426d950898",
   "metadata": {},
   "source": [
    "## Założenia\n",
    "1. Zaznajomić się z problemem deep double descent, opisanym na slajdach powyżej i cytowanym na slajdzie tytułowym artykule.\n",
    "2. Wybrać dowolny zbiór danych o niewielkim wymiarze (M~10) i próbkach (wektorach) o współrzędnych (cechach) numerycznych. Liczność zbioru też bez przesady M~5000, .dane wieloklasowe. Zbiór testowy ok 20%. Proszę by zbiory się nie powtarzały by otrzymać lepszą statystykę.\n",
    "3. Skupiamy się na architekturze sieci MLP (vanilla NN ze standardowymi parametrami) i zakładamy, że ilość parametrów sieci jest stała: (a) mniejsza od ilości danych; b) w przybliżeniu równa c) dużo większa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7ab9da0f0917b",
   "metadata": {},
   "source": [
    "### Przygotowanie datasetu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51a68396baabfb",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:22:55.850375Z",
     "start_time": "2025-06-03T08:22:54.215230Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ffc336dfe5a0430c",
   "metadata": {},
   "source": [
    "load dataset \"Letter Recognition\"\n",
    "\n",
    "- Features: 16\n",
    "- Samples: use a random subset of 5000 for manageable size\n",
    "- Classes: 26 letter labels"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ff628ab7e2a6406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:22:55.895894Z",
     "start_time": "2025-06-03T08:22:55.854391Z"
    }
   },
   "source": [
    "letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "X_full, y_full = letter.data, letter.target\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(X_full), 5000, replace=False)\n",
    "X, y = X_full[indices], y_full[indices]\n",
    "\n",
    "#Function to get a fresh train/test split\n",
    "def get_split(random_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "    )\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c3d65d71deb2a580",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "Dla KAŻDEJ z trzech sieci o różnej złożoności, zbadać jak zachowują się (ACC, F1) dla jej różnych realizacji czyli: a) sieć z jedną warstwą ukrytą, 2) dwoma 3) trzema itd.. Każdy wynik powtórzyć co najmniej 3 razy i wyliczyć średnią. Dla uproszczenia możemy przyjąć że ilość neuronów w warstwach ukrytych jest taka sama. Zakładamy, że CAŁKOWITA liczba neuronów jest taka sama (jak w założeniu 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "747eeec50fe59706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:26:31.677232Z",
     "start_time": "2025-06-03T08:22:56.392730Z"
    }
   },
   "source": [
    "# task 1\n",
    "def build_mlp(n_layers, total_neurons, random_state=None):\n",
    "    neurons = max(total_neurons // n_layers, 1)\n",
    "    hidden_layer_sizes = tuple([neurons] * n_layers)\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate='adaptive',\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=10000,\n",
    "        tol=1e-4,\n",
    "        random_state=random_state,\n",
    "        verbose=False\n",
    "    )\n",
    "    return model\n",
    "\n",
    "results = []\n",
    "total_neurons_list = [10, 20, 50]\n",
    "for total_neurons in total_neurons_list:\n",
    "    for n_layers in [1, 2, 3]:\n",
    "        for repeat in range(3):\n",
    "            X_train, X_test, y_train, y_test = get_split(random_state=repeat)\n",
    "            model = build_mlp(n_layers, total_neurons, random_state=repeat)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            results.append({\n",
    "                'total_neurons': total_neurons,\n",
    "                'n_layers': n_layers,\n",
    "                'repeat': repeat,\n",
    "                'accuracy': acc,\n",
    "                'f1_macro': f1\n",
    "            })"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7797d5ad5f071aaa",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b86e311f00287b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:26:31.716984Z",
     "start_time": "2025-06-03T08:26:31.685811Z"
    }
   },
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "summary = results_df.groupby(['total_neurons', 'n_layers'])[['accuracy', 'f1_macro']].agg(['mean', 'std']).reset_index()\n",
    "print(summary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  total_neurons n_layers  accuracy            f1_macro          \n",
      "                              mean       std      mean       std\n",
      "0            10        1  0.784000  0.021000  0.783222  0.020384\n",
      "1            10        2  0.631333  0.011590  0.627433  0.012280\n",
      "2            10        3  0.340667  0.119182  0.305594  0.141511\n",
      "3            20        1  0.840000  0.010392  0.838942  0.010538\n",
      "4            20        2  0.777000  0.007211  0.776641  0.006894\n",
      "5            20        3  0.686667  0.025541  0.688821  0.028471\n",
      "6            50        1  0.888000  0.013892  0.887846  0.013465\n",
      "7            50        2  0.867000  0.004583  0.866333  0.004117\n",
      "8            50        3  0.840000  0.016643  0.839137  0.017208\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wnioski z zadania 1\n",
    "Zgodnie z przedstawionymi wynikami, dla ustalonej całkowitej liczby neuronów, zwiększanie liczby warstw (co prowadzi do zmniejszenia liczby neuronów w pojedynczej warstwie) generalnie pogarszało wydajność modelu (zarówno metrykę accuracy, jak i F1-macro). Efekt ten był szczególnie wyraźny przy mniejszej całkowitej liczbie neuronów (np. 10 neuronów). Przy większej liczbie neuronów (np. 50) spadek wydajności był mniej stromy, ale nadal zauważalny.\n",
    "\n",
    "Kluczowe obserwacje:\n",
    "\n",
    "- 10 neuronów:\n",
    "\n",
    "    1 warstwa: Accuracy ~0.78\n",
    "    \n",
    "    3 warstwy: Accuracy ~0.34\n",
    "- 50 neuronów:\n",
    "\n",
    "    1 warstwa: Accuracy ~0.88\n",
    "    \n",
    "    3 warstwy: Accuracy ~0.84\n",
    "Sugeruje to, że zbyt \"cienkie\" warstwy, nawet jeśli jest ich więcej, mogą nie być w stanie efektywnie przetwarzać informacji.\n",
    "\n"
   ],
   "id": "17c4fe61f02f5c6c"
  },
  {
   "cell_type": "markdown",
   "id": "24118ccd973c8331",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Co zaobserwujemy zwiększając liczbę warstw przy ustalonej ilości parametrów sieci? Wysnuć hipotezę dlaczego tak się może dziać i czy istnieje jakiś element architektury którego zmiana zmieniłaby tą sytuację. Rekomenduję wykorzystanie obliczenia saturacji w poszczególnych warstwach.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a207d99f4a49805a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:31:04.213630Z",
     "start_time": "2025-06-03T08:28:32.956786Z"
    }
   },
   "source": [
    "# task 2\n",
    "def get_hidden_activations(model, X):\n",
    "    hidden_layer_sizes = model.hidden_layer_sizes\n",
    "    if not hasattr(hidden_layer_sizes, '__iter__'):\n",
    "        hidden_layer_sizes = [hidden_layer_sizes]\n",
    "    \n",
    "    activations_list = []\n",
    "    current_input = X\n",
    "    \n",
    "    for i in range(model.n_layers_ - 2):\n",
    "        raw_activation = current_input @ model.coefs_[i] + model.intercepts_[i]\n",
    "        \n",
    "        if model.activation == 'relu':\n",
    "            activated_output = np.maximum(0, raw_activation)\n",
    "        elif model.activation == 'tanh':\n",
    "            activated_output = np.tanh(raw_activation)\n",
    "        elif model.activation == 'logistic':\n",
    "            activated_output = 1 / (1 + np.exp(-raw_activation))\n",
    "        elif model.activation == 'identity':\n",
    "            activated_output = raw_activation\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {model.activation}\")\n",
    "            \n",
    "        activations_list.append(activated_output)\n",
    "        current_input = activated_output\n",
    "            \n",
    "    return activations_list\n",
    "\n",
    "def calculate_relu_saturation(model, X):\n",
    "    if model.activation != 'relu':\n",
    "        return [np.nan] * len(model.hidden_layer_sizes if hasattr(model.hidden_layer_sizes, '__iter__') else [model.hidden_layer_sizes])\n",
    "\n",
    "    hidden_activations = get_hidden_activations(model, X)\n",
    "    \n",
    "    saturations_per_layer = []\n",
    "    for layer_acts in hidden_activations:\n",
    "        mean_layer_saturation = np.mean(layer_acts == 0)\n",
    "        saturations_per_layer.append(mean_layer_saturation)\n",
    "        \n",
    "    return saturations_per_layer\n",
    "\n",
    "saturation_analysis_results = []\n",
    "total_neurons_configs_for_sat = [10, 50] \n",
    "n_layers_configs_for_sat = [1, 2, 3]\n",
    "\n",
    "print(\"Calculating saturation for selected configurations...\")\n",
    "for total_neurons in total_neurons_configs_for_sat:\n",
    "    for n_layers in n_layers_configs_for_sat:\n",
    "        avg_sats_for_config_repeats = []\n",
    "        detailed_sats_for_config_repeats = [] \n",
    "        \n",
    "        for repeat in range(3):\n",
    "            print(f\"  Total neurons: {total_neurons}, Layers: {n_layers}, Repeat: {repeat}\")\n",
    "            X_train_sat, X_test_sat, y_train_sat, _ = get_split(random_state=repeat)\n",
    "            \n",
    "            if total_neurons // n_layers < 1 and n_layers > 0 :\n",
    "                 print(f\"    Skipping {total_neurons} total neurons / {n_layers} layers as neurons per layer < 1 before correction.\")\n",
    "                 continue\n",
    "\n",
    "            model_for_saturation = build_mlp(n_layers, total_neurons, random_state=repeat)\n",
    "            model_for_saturation.fit(X_train_sat, y_train_sat)\n",
    "            \n",
    "            layer_saturations = calculate_relu_saturation(model_for_saturation, X_test_sat)\n",
    "            \n",
    "            saturation_analysis_results.append({\n",
    "                'total_neurons': total_neurons,\n",
    "                'n_layers': n_layers,\n",
    "                'repeat': repeat,\n",
    "                'layer_saturations': layer_saturations,\n",
    "                'mean_network_saturation': np.mean(layer_saturations) if layer_saturations else np.nan \n",
    "            })\n",
    "\n",
    "saturation_df = pd.DataFrame(saturation_analysis_results)\n",
    "\n",
    "print(\"\\nSaturation Analysis Summary (Mean Network Saturation - averaged over layers and repeats):\")\n",
    "summary_saturation = saturation_df.groupby(['total_neurons', 'n_layers'])['mean_network_saturation'].agg(['mean', 'std']).reset_index()\n",
    "print(summary_saturation)\n",
    "\n",
    "print(\"\\nDetailed Per-Layer Saturation (example for one repeat, Total Neurons=50):\")\n",
    "for n_layers_val in n_layers_configs_for_sat:\n",
    "    detail = saturation_df[\n",
    "        (saturation_df['total_neurons'] == 50) & \n",
    "        (saturation_df['n_layers'] == n_layers_val) &\n",
    "        (saturation_df['repeat'] == 0)\n",
    "    ]\n",
    "    if not detail.empty:\n",
    "        print(f\"  Total Neurons: 50, Layers: {n_layers_val}, Repeat 0, Per-Layer Saturations: {detail['layer_saturations'].values[0]}\")\n",
    "\n",
    "print(\"\\nDetailed Per-Layer Saturation (example for one repeat, Total Neurons=10):\")\n",
    "for n_layers_val in n_layers_configs_for_sat:\n",
    "    detail = saturation_df[\n",
    "        (saturation_df['total_neurons'] == 10) & \n",
    "        (saturation_df['n_layers'] == n_layers_val) &\n",
    "        (saturation_df['repeat'] == 0)\n",
    "    ]\n",
    "    if not detail.empty:\n",
    "        neurons_per_layer = max(10 // n_layers_val, 1)\n",
    "        print(f\"  Total Neurons: 10, Layers: {n_layers_val} ({neurons_per_layer} neurons/layer), Repeat 0, Per-Layer Saturations: {detail['layer_saturations'].values[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating saturation for selected configurations...\n",
      "  Total neurons: 10, Layers: 1, Repeat: 0\n",
      "  Total neurons: 10, Layers: 1, Repeat: 1\n",
      "  Total neurons: 10, Layers: 1, Repeat: 2\n",
      "  Total neurons: 10, Layers: 2, Repeat: 0\n",
      "  Total neurons: 10, Layers: 2, Repeat: 1\n",
      "  Total neurons: 10, Layers: 2, Repeat: 2\n",
      "  Total neurons: 10, Layers: 3, Repeat: 0\n",
      "  Total neurons: 10, Layers: 3, Repeat: 1\n",
      "  Total neurons: 10, Layers: 3, Repeat: 2\n",
      "  Total neurons: 50, Layers: 1, Repeat: 0\n",
      "  Total neurons: 50, Layers: 1, Repeat: 1\n",
      "  Total neurons: 50, Layers: 1, Repeat: 2\n",
      "  Total neurons: 50, Layers: 2, Repeat: 0\n",
      "  Total neurons: 50, Layers: 2, Repeat: 1\n",
      "  Total neurons: 50, Layers: 2, Repeat: 2\n",
      "  Total neurons: 50, Layers: 3, Repeat: 0\n",
      "  Total neurons: 50, Layers: 3, Repeat: 1\n",
      "  Total neurons: 50, Layers: 3, Repeat: 2\n",
      "\n",
      "Saturation Analysis Summary (Mean Network Saturation - averaged over layers and repeats):\n",
      "   total_neurons  n_layers      mean       std\n",
      "0             10         1  0.298700  0.036386\n",
      "1             10         2  0.224900  0.057764\n",
      "2             10         3  0.372963  0.124642\n",
      "3             50         1  0.414287  0.004240\n",
      "4             50         2  0.317613  0.007329\n",
      "5             50         3  0.277604  0.011406\n",
      "\n",
      "Detailed Per-Layer Saturation (example for one repeat, Total Neurons=50):\n",
      "  Total Neurons: 50, Layers: 1, Repeat 0, Per-Layer Saturations: [np.float64(0.41002)]\n",
      "  Total Neurons: 50, Layers: 2, Repeat 0, Per-Layer Saturations: [np.float64(0.42088), np.float64(0.2308)]\n",
      "  Total Neurons: 50, Layers: 3, Repeat 0, Per-Layer Saturations: [np.float64(0.3516875), np.float64(0.237375), np.float64(0.2055625)]\n",
      "\n",
      "Detailed Per-Layer Saturation (example for one repeat, Total Neurons=10):\n",
      "  Total Neurons: 10, Layers: 1 (10 neurons/layer), Repeat 0, Per-Layer Saturations: [np.float64(0.2706)]\n",
      "  Total Neurons: 10, Layers: 2 (5 neurons/layer), Repeat 0, Per-Layer Saturations: [np.float64(0.25), np.float64(0.0758)]\n",
      "  Total Neurons: 10, Layers: 3 (3 neurons/layer), Repeat 0, Per-Layer Saturations: [np.float64(0.431), np.float64(0.3396666666666667), np.float64(0.006)]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wnioski z zadania 2\n",
    "Obliczenia saturacji dla neuronów ReLU (mierzonej jako odsetek neuronów zwracających zero) dostarczyły dodatkowych informacji:\n",
    "\n",
    "Dla 50 neuronów ogółem:\n",
    "\n",
    "    1 warstwa: saturacja ~0.41\n",
    "    \n",
    "    2 warstwy: saturacje ~[0.42, 0.23]\n",
    "    \n",
    "    3 warstwy: saturacje ~[0.35, 0.23, 0.20]\n",
    "    \n",
    "Dla 10 neuronów ogółem:\n",
    "\n",
    "    1 warstwa (10 neuronów/warstwę): saturacja ~0.27\n",
    "    \n",
    "    2 warstwy (5 neuronów/warstwę): saturacje ~[0.25, 0.07]\n",
    "    \n",
    "    3 warstwy (3 neurony/warstwę): saturacje ~[0.43, 0.33, 0.006]\n",
    "\n",
    "***Interpretacja:***\n",
    "\n",
    "W przypadku 50 neuronów, średnia saturacja w warstwach wydaje się maleć lub utrzymywać na podobnym poziomie w głębszych warstwach, co może oznaczać, że neurony pozostają aktywne. Jednak w konfiguracji z 10 neuronami i 3 warstwami, pierwsza warstwa wykazuje stosunkowo wysoką saturację (~0.43), co może wskazywać na \"umieranie\" neuronów i utratę informacji już na wczesnym etapie, co koreluje z drastycznym spadkiem wydajności obserwowanym w Zadaniu 1. Ostatnia warstwa (3 neurony) w tym przypadku wykazuje bardzo niską saturację (~0.006), co może oznaczać, że te nieliczne neurony są bardzo aktywne, ale jest ich zbyt mało, by efektywnie przetworzyć dane. Ogólnie, głębsze i cieńsze sieci mogą być bardziej podatne na problemy z przepływem informacji i aktywnością neuronów, co przekłada się na gorsze wyniki."
   ],
   "id": "83eec08cda13eebc"
  },
  {
   "cell_type": "markdown",
   "id": "5b83fa9cdf56889b",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "W kontekście deep double descent proszę zobaczyć co dzieje się gdy analizujemy wyniki z punktu (2) dla 3 sieci o różnych złożonościach z zaszumionymi etykietami."
   ]
  },
  {
   "cell_type": "code",
   "id": "175c04ec7bd92cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:35:16.638574Z",
     "start_time": "2025-06-03T08:32:02.871634Z"
    }
   },
   "source": [
    "# task 3\n",
    "def add_label_noise(y_train_orig, noise_fraction, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    y_train_noisy = np.copy(y_train_orig)\n",
    "    unique_labels = np.unique(y_train_orig)\n",
    "    n_labels = len(y_train_noisy)\n",
    "    n_to_flip = int(noise_fraction * n_labels)\n",
    "    \n",
    "    if n_to_flip == 0:\n",
    "        return y_train_noisy\n",
    "\n",
    "    flip_indices = np.random.choice(np.arange(n_labels), size=n_to_flip, replace=False)\n",
    "    \n",
    "    for idx in flip_indices:\n",
    "        original_label = y_train_noisy[idx]\n",
    "        possible_new_labels = [l for l in unique_labels if l != original_label]\n",
    "        if not possible_new_labels:\n",
    "            continue \n",
    "        new_label = np.random.choice(possible_new_labels)\n",
    "        y_train_noisy[idx] = new_label\n",
    "        \n",
    "    return y_train_noisy\n",
    "\n",
    "noise_level = 0.2\n",
    "results_noisy = []\n",
    "total_neurons_list_task3 = [10, 20, 50]\n",
    "n_layers_list_task3 = [1, 2, 3]\n",
    "\n",
    "print(f\"\\nRunning experiments with {noise_level*100}% label noise...\")\n",
    "\n",
    "for total_neurons in total_neurons_list_task3:\n",
    "    for n_layers in n_layers_list_task3:\n",
    "        acc_repeats = []\n",
    "        f1_repeats = []\n",
    "        for repeat in range(3): # 3 repeats\n",
    "            print(f\"  Total neurons: {total_neurons}, Layers: {n_layers}, Repeat: {repeat} (with noise)\")\n",
    "\n",
    "            X_train, X_test, y_train_clean, y_test_clean = get_split(random_state=repeat)\n",
    "            \n",
    "            y_train_noisy = add_label_noise(y_train_clean, noise_level, random_state=(42 + repeat))\n",
    "            \n",
    "            model_noisy = build_mlp(n_layers, total_neurons, random_state=repeat)\n",
    "            model_noisy.fit(X_train, y_train_noisy)\n",
    "            \n",
    "            y_pred_noisy = model_noisy.predict(X_test)\n",
    "            \n",
    "            acc = accuracy_score(y_test_clean, y_pred_noisy)\n",
    "            f1 = f1_score(y_test_clean, y_pred_noisy, average='macro', zero_division=0)\n",
    "            \n",
    "            results_noisy.append({\n",
    "                'total_neurons': total_neurons,\n",
    "                'n_layers': n_layers,\n",
    "                'repeat': repeat,\n",
    "                'noise_level': noise_level,\n",
    "                'accuracy': acc,\n",
    "                'f1_macro': f1\n",
    "            })\n",
    "\n",
    "results_noisy_df = pd.DataFrame(results_noisy)\n",
    "\n",
    "print(\"\\nSummary of Results with Noisy Labels:\")\n",
    "summary_noisy = results_noisy_df.groupby(['total_neurons', 'n_layers', 'noise_level'])[['accuracy', 'f1_macro']].agg(['mean', 'std']).reset_index()\n",
    "print(summary_noisy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiments with 20.0% label noise...\n",
      "  Total neurons: 10, Layers: 1, Repeat: 0 (with noise)\n",
      "  Total neurons: 10, Layers: 1, Repeat: 1 (with noise)\n",
      "  Total neurons: 10, Layers: 1, Repeat: 2 (with noise)\n",
      "  Total neurons: 10, Layers: 2, Repeat: 0 (with noise)\n",
      "  Total neurons: 10, Layers: 2, Repeat: 1 (with noise)\n",
      "  Total neurons: 10, Layers: 2, Repeat: 2 (with noise)\n",
      "  Total neurons: 10, Layers: 3, Repeat: 0 (with noise)\n",
      "  Total neurons: 10, Layers: 3, Repeat: 1 (with noise)\n",
      "  Total neurons: 10, Layers: 3, Repeat: 2 (with noise)\n",
      "  Total neurons: 20, Layers: 1, Repeat: 0 (with noise)\n",
      "  Total neurons: 20, Layers: 1, Repeat: 1 (with noise)\n",
      "  Total neurons: 20, Layers: 1, Repeat: 2 (with noise)\n",
      "  Total neurons: 20, Layers: 2, Repeat: 0 (with noise)\n",
      "  Total neurons: 20, Layers: 2, Repeat: 1 (with noise)\n",
      "  Total neurons: 20, Layers: 2, Repeat: 2 (with noise)\n",
      "  Total neurons: 20, Layers: 3, Repeat: 0 (with noise)\n",
      "  Total neurons: 20, Layers: 3, Repeat: 1 (with noise)\n",
      "  Total neurons: 20, Layers: 3, Repeat: 2 (with noise)\n",
      "  Total neurons: 50, Layers: 1, Repeat: 0 (with noise)\n",
      "  Total neurons: 50, Layers: 1, Repeat: 1 (with noise)\n",
      "  Total neurons: 50, Layers: 1, Repeat: 2 (with noise)\n",
      "  Total neurons: 50, Layers: 2, Repeat: 0 (with noise)\n",
      "  Total neurons: 50, Layers: 2, Repeat: 1 (with noise)\n",
      "  Total neurons: 50, Layers: 2, Repeat: 2 (with noise)\n",
      "  Total neurons: 50, Layers: 3, Repeat: 0 (with noise)\n",
      "  Total neurons: 50, Layers: 3, Repeat: 1 (with noise)\n",
      "  Total neurons: 50, Layers: 3, Repeat: 2 (with noise)\n",
      "\n",
      "Summary of Results with Noisy Labels:\n",
      "  total_neurons n_layers noise_level  accuracy            f1_macro          \n",
      "                                          mean       std      mean       std\n",
      "0            10        1         0.2  0.732667  0.015567  0.729648  0.015544\n",
      "1            10        2         0.2  0.601667  0.010263  0.580670  0.020354\n",
      "2            10        3         0.2  0.273667  0.135220  0.200170  0.127428\n",
      "3            20        1         0.2  0.795000  0.014107  0.795118  0.015039\n",
      "4            20        2         0.2  0.736000  0.028054  0.735917  0.030036\n",
      "5            20        3         0.2  0.648333  0.029838  0.640672  0.035701\n",
      "6            50        1         0.2  0.823667  0.018903  0.823391  0.018590\n",
      "7            50        2         0.2  0.818333  0.008963  0.817703  0.009166\n",
      "8            50        3         0.2  0.785333  0.007767  0.785478  0.010475\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wnioski z zadania 3\n",
    "Wprowadzenie 20% szumu do etykiet treningowych spowodowało oczekiwany spadek wydajności (accuracy i F1-macro) we wszystkich testowanych konfiguracjach w porównaniu do wyników z Zadania 1.\n",
    "\n",
    "10 neuronów, 1 warstwa:\n",
    "\n",
    "    Accuracy spadło z ~0.78 do ~0.73\n",
    "10 neuronów, 3 warstwy:\n",
    "    \n",
    "    Accuracy spadło z ~0.34 do ~0.27\n",
    "50 neuronów, 1 warstwa:\n",
    "    \n",
    "    Accuracy spadło z ~0.88 do ~0.82\n",
    "50 neuronów, 3 warstwy:\n",
    "    \n",
    "    Accuracy spadło z ~0.84 do ~0.78\n",
    "Tendencja obserwowana w Zadaniu 1 (pogarszanie się wyników wraz ze wzrostem liczby warstw przy stałej liczbie neuronów) została zachowana, a nawet w niektórych przypadkach procentowy spadek wydajności był bardziej dotkliwy. Modele trenowane na zaszumionych danych miały trudniejsze zadanie generalizacji, a ograniczenia związane ze zbyt cienkimi warstwami stały się jeszcze bardziej widoczne. Nie zaobserwowano wyraźnego zjawiska \"deep double descent\" w kontekście zmiany liczby warstw dla ustalonej liczby neuronów, raczej ogólne pogorszenie zdolności generalizacji przez modele."
   ],
   "id": "de473f7cf170f414"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "262f9f493c0aa627"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
